#Architecture
book_recommender/
├── data/
│   ├── raw/                  # Original raw data files
│   │   ├── books.csv
│   ├── processed/            # Processed data files
│   │   ├── book_cleaned.csv
│   │   ├── books_with_categories.csv
│   │   ├── books_with_emotions.csv
│   │   ├── tagged_description.txt
│   ├── assets/               # Static files
│   │   ├── cover-not-found.jpg
│
├── scripts/
│   ├── data_processing/      # Data manipulation scripts
│   │   ├── data_exploration.py
│   │   ├── sentiment_analysis.py
│   │   ├── text_classification.py
│   │   ├── vector_search.py
│   │   ├── __init__.py (empty)
│
├── app/                     # Application files
│   ├── gradio_dashboard.py   # Main application
│   ├── __init__.py (empty)
│
├── requirements.txt          # Python dependencies
├── setup.txt                 # Setup instructions
├── README.md                 # Project documentation
├── .gitignore
├── .env 


//1 libraries
pip install pandas -> Work with tabular data
pip install matplotlib seaborn -> For visualization
pip install numpy -> for Numerical computing
pip install python-dotenv -> For API credential access
pip install langchain-core langchain-community -> Core LangChain + Community Packages
pip install langchain-text-splitters -> Text Loaders & Splitters (for TextLoader and CharacterTextSplitter)
pip install sentence-transformers -> Hugging Face Embeddings (for HuggingFaceEmbeddings)
pip install transformers -> for pipeline
pip install chromadb -> Chroma Vector Store (for Chroma)
pip install huggingface-hub -> Hugging Face Hub (if using HF models)
pip install gradio -> Framwork to interact with recommender
pip install tqdm -> for progress bars

//2 dataset
https://www.kaggle.com/datasets/dylanjcastillo/7k-books-with-metadata
done exploration through-> data_exploration.py
Findings:
>>All unique isbn (No repeated books)
>>subtitle column has lots of missing observations (4429/6810)
>>Lots of categories (567) with uneven distribution (Majority Fiction) -> Long Tail Problem
>>Some description have very words making them meaningless
Done:
>>deleted missing observations (~300 deleted)
>>Removed books with minimum words in description < 25 (5197 Remains)
>>added taged description field for easy searching
>>Saving-> books_cleaned.scv
*Pearson when dealing with continuous values and spearman when non-continuous

//3 Word embedding on clean dataset
>>Used LangChain for vector search
>>HuggingFace for converting chunks to embedding
>>Chroma database for storing embedding
>>Main recommendation through:
def retrieve_semantic_recommendations(
        query: str,
        top_k: int = 10,
) -> pd.DataFrame:
    recs = db_books.similarity_search(query, k = 50)
    books_list = []
    for i in range(0, len(recs)):
        books_list += [int(recs[i].page_content.strip('"').split()[0])]
    return books[books["isbn13"].isin(books_list)]
retrieve_semantic_recommendations("A book to teach children about nature")
>>Saving in tagged_description.txt

//4 Text classification
>>Filtering popular categories (Entries > 50)
>>Simplifying complex categories into fiction, non-fiction and children category
>>Used Zero-Shot classification by bart ( a machine learning technique where a model can classify data into 
categories it has never explicitly been trained on. It leverage pre-existing knowledge to make 
predictions without fine-tuning.)
>>Model validation -> 78% accuracy
>>Updating categories with model predictions where missing
>>Saving file-> books_with_categories.csv

//5 Sentiment Analysis
>>Setted up text classification pipeline to predict emotions from text
>>Used Emotion English DistilRoBERTa-base model for fine tuning from huggingface (66% accuracy)
>>Function to calculate max emotion scores
emotion_labels = ["anger", "disgust", "fear", "joy", "sadness", "surprise", "neutral"]
isbn = []
emotion_scores = {label: [] for label in emotion_labels}
def calculate_max_emotion_scores(predictions):
    per_emotion_scores = {label: [] for label in emotion_labels}
    for prediction in predictions:
        sorted_predictions = sorted(prediction, key=lambda x: x["label"])
        for index, label in enumerate(emotion_labels):
            per_emotion_scores[label].append(sorted_predictions[index]["score"])
    return {label: np.max(scores) for label, scores in per_emotion_scores.items()}
>>Created a dataframe for emotion scores
>>Merged emotions back into original data and saved it-> books_with_emotions.csv

//6 Gradio dashboard
>>Loading book data from books_with_emotions.csv
>>Taking higher resolution of thumbnail url
>>Giving default image to missing thumbnail
>>Loading text data from "tagged_description.txt"
>>Splitting documents into chunks (line by line)
>>Creating a Chroma vector database from the documents
>>Recommendation System:
retrieve_semantic_recommendations():
    Finds similar books using vector similarity search
    Filters by category if specified
    Sorts by emotional tone (joy, surprise, anger, fear, sadness)
recommend_books():
    Processes recommendations into display format
    Formats author lists and truncates descriptions
    Prepares thumbnails and captions for display
>>UI Creation
input components:
    Textbox for book description queries
    Dropdowns for category and emotional tone selection
output component:
    Gallery to display book recommendations (8 columns × 2 rows)
Connecting button click to recommendation function

*****Overall Purpose*****
Uses semantic search to find relevant books
Allows filtering by category
Can prioritize books with specific emotional tones
Presents results in a visually appealing gallery format